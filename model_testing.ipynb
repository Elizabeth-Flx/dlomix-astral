{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356ac79e-56e5-49ab-b003-ab254a481829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 20:01:21.123200: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-28 20:01:21.177630: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-28 20:01:21.177677: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-28 20:01:21.177715: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-28 20:01:21.187236: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-28 20:01:30.969396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "373aabd8-d89c-44c3-b0bd-127d73fc3d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.layers.experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerModel \n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerModel(\n\u001b[1;32m      4\u001b[0m     running_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      5\u001b[0m     d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     inject_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# all | pre | post    \u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdlomix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m masked_spectral_distance, masked_pearson_correlation_distance\n",
      "File \u001b[0;32m~/projects/astral/dlomix/my_scripts/models/models.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m L \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mlayers\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_parts\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[1;32m      7\u001b[0m ALPHABET_UNMOD \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     28\u001b[0m }\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformerModel\u001b[39;00m(K\u001b[38;5;241m.\u001b[39mModel):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental'"
     ]
    }
   ],
   "source": [
    "from models.models import TransformerModel \n",
    "\n",
    "model = TransformerModel(\n",
    "    running_units=256,\n",
    "    d=64,\n",
    "    h=4,\n",
    "    ffn_mult=1,\n",
    "    depth=3,\n",
    "    pos_type='learned', # learned\n",
    "    prec_type=\"embed_input\", # embed_input | pretoken | inject_pre | inject_ffn\n",
    "    learned_pos=True,\n",
    "    prenorm=False,\n",
    "    norm_type=\"layer\",\n",
    "    penultimate_units=None,\n",
    "    output_units=174,\n",
    "    max_charge=6,\n",
    "    sequence_length=30,\n",
    "    alphabet=False,\n",
    "    dropout=0,\n",
    "    precursor_units=None,\n",
    "    inject_position=\"all\" # all | pre | post    \n",
    ")\n",
    "\n",
    "\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer='adam', \n",
    "            loss=masked_spectral_distance,\n",
    "            metrics=[masked_pearson_correlation_distance])\n",
    "\n",
    "model.build(input_shape=(None, 30))\n",
    "\n",
    "model.summary()\n",
    "#models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49447bd-168d-451a-8535-066a2b103427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/students/d.lochert/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliable feature extractors are (use the key of the following dict and pass it to features_to_extract in the Dataset Class):\n",
      "{\n",
      "   \"atom_count\": \"Atom count of PTM.\",\n",
      "   \"delta_mass\": \"Delta mass of PTM.\",\n",
      "   \"mod_gain\": \"Gain of atoms due to PTM.\",\n",
      "   \"mod_loss\": \"Loss of atoms due to PTM.\"\n",
      "}.\n",
      "When writing your own feature extractor, you can either\n",
      "    (1) use the FeatureExtractor class or\n",
      "    (2) write a function that can be mapped to the Hugging Face dataset.\n",
      "In both cases, you can access the parsed sequence information from the dataset using the following keys, which all provide python lists:\n",
      "    - _parsed_sequence: The parsed sequence\n",
      "    - _n_term_mods: The N-terminal modifications\n",
      "    - _c_term_mods: The C-terminal modifications\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/students/d.lochert/projects/astral/dlomix/src/dlomix/data/dataset.py:169: UserWarning: \n",
      "                    Multiple data sources provided {'train': '/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean_train.parquet', 'val': '/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean_val.parquet', 'test': '/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean_test.parquet'}, please ensure that the data sources are already split into train, val and test sets\n",
      "                    since no splitting will happen. If not, please provide only one data source and set the val_ratio to split the data into train and val sets.\"\n",
      "                    \n",
      "  warnings.warn(\n",
      "Mapping SequenceParsingProcessor (num_proc=2): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 2836/2836 [00:00<00:00, 12543.77 examples/s]\n",
      "Mapping SequenceParsingProcessor (num_proc=2): 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:00<00:00, 4626.43 examples/s]\n",
      "Mapping SequenceParsingProcessor (num_proc=2): 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 402/402 [00:00<00:00, 2102.48 examples/s]\n",
      "Mapping SequenceEncodingProcessor (num_proc=2): 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2836/2836 [00:00<00:00, 10963.28 examples/s]\n",
      "Mapping SequenceEncodingProcessor (num_proc=2): 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:00<00:00, 4025.89 examples/s]\n",
      "Mapping SequenceEncodingProcessor (num_proc=2): 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 402/402 [00:00<00:00, 2273.74 examples/s]\n",
      "Mapping SequencePaddingProcessor (num_proc=2): 100%|████████████████████████████████████████████████████████████████████████████████████████████| 2836/2836 [00:00<00:00, 12217.67 examples/s]\n",
      "Mapping SequencePaddingProcessor (num_proc=2): 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:00<00:00, 3997.92 examples/s]\n",
      "Mapping SequencePaddingProcessor (num_proc=2): 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 402/402 [00:00<00:00, 1490.77 examples/s]\n",
      "Filter (num_proc=2): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2836/2836 [00:00<00:00, 15949.48 examples/s]\n",
      "Filter (num_proc=2): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 814/814 [00:00<00:00, 4282.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Prosit dataset\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = \"/cmnfs/proj/prosit_astral\"\n",
    "os.environ['HF_DATASETS_CACHE'] = \"/cmnfs/proj/prosit_astral/datasets\"\n",
    "\n",
    "print(\"[UNIMOD:1]-K[UNIMOD:1]\".count('[UNIMOD:' + '1' + ']'))\n",
    "\n",
    "import numpy as np\n",
    "from dlomix.data import FragmentIonIntensityDataset\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import disable_caching\n",
    "disable_caching()\n",
    "\n",
    "PTMS_ALPHABET = {\n",
    "    \"A\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "    \"F\": 5,\n",
    "    \"G\": 6,\n",
    "    \"H\": 7,\n",
    "    \"I\": 8,\n",
    "    \"K\": 9,\n",
    "    \"L\": 10,\n",
    "    \"M\": 11,\n",
    "    \"N\": 12,\n",
    "    \"P\": 13,\n",
    "    \"Q\": 14,\n",
    "    \"R\": 15,\n",
    "    \"S\": 16,\n",
    "    \"T\": 17,\n",
    "    \"V\": 18,\n",
    "    \"W\": 19,\n",
    "    \"Y\": 20,\n",
    "    \"M[UNIMOD:35]\": 21,\n",
    "    \"R[UNIMOD:7]\":22,\n",
    "    \"C[UNIMOD:4]\": 2,\n",
    "    \"Q[UNIMOD:7]\":4,\n",
    "    \"N[UNIMOD:7]\":3,\n",
    "}\n",
    "\n",
    "rt_data = FragmentIonIntensityDataset(\n",
    "    data_source=\"/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean_train.parquet\",\n",
    "    val_data_source=\"/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean_val.parquet\",\n",
    "    test_data_source=\"/cmnfs/data/proteomics/Prosit_PTMs/Transformer_Train/clean_test.parquet\",\n",
    "    data_format=\"parquet\", \n",
    "    val_ratio=0.2, max_seq_len=30, encoding_scheme=\"naive-mods\",\n",
    "    vocab=PTMS_ALPHABET,\n",
    "    model_features=[\"precursor_charge_onehot\", \"collision_energy_aligned_normed\",\"method_nbr\"],\n",
    "    batch_size=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17f28420-a558-46c9-9752-09e6d9bd541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['modified_sequence', 'intensities_raw', 'precursor_charge_onehot', 'collision_energy_aligned_normed', 'method_nbr', '_parsed_sequence', '_n_term_mods', '_c_term_mods'],\n",
       "    num_rows: 2836\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bcedee3-07a0-4cbf-8114-48f71de9bcbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=({'modified_sequence': TensorSpec(shape=(None, 30), dtype=tf.float32, name=None), 'precursor_charge_onehot': TensorSpec(shape=(None, 6), dtype=tf.int64, name=None), 'collision_energy_aligned_normed': TensorSpec(shape=(None,), dtype=tf.float32, name=None), 'method_nbr': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 174), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_data.tensor_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "341972ea-7025-447a-a8ac-ea5d0018e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 16s 4s/step - loss: 0.7404 - masked_pearson_correlation_distance: 0.6821 - val_loss: 0.7023 - val_masked_pearson_correlation_distance: 0.6291\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.7113 - masked_pearson_correlation_distance: 0.6377 - val_loss: 0.6785 - val_masked_pearson_correlation_distance: 0.5939\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 8s 2s/step - loss: 0.6879 - masked_pearson_correlation_distance: 0.6046 - val_loss: 0.6720 - val_masked_pearson_correlation_distance: 0.5884\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 8s 3s/step - loss: 0.6688 - masked_pearson_correlation_distance: 0.5802 - val_loss: 0.6615 - val_masked_pearson_correlation_distance: 0.5764\n",
      "Model: \"transformer_model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " string_lookup_20 (StringLo  multiple                  0 (unused)\n",
      " okup)                                                           \n",
      "                                                                 \n",
      " dense_120 (Dense)           multiple                  6656      \n",
      "                                                                 \n",
      " dense_121 (Dense)           multiple                  1792      \n",
      "                                                                 \n",
      " precursor_token_15 (Precur  multiple                  65792     \n",
      " sorToken)                                                       \n",
      "                                                                 \n",
      " trans_block_45 (TransBlock  multiple                  394496    \n",
      " )                                                               \n",
      "                                                                 \n",
      " trans_block_46 (TransBlock  multiple                  394496    \n",
      " )                                                               \n",
      "                                                                 \n",
      " trans_block_47 (TransBlock  multiple                  394496    \n",
      " )                                                               \n",
      "                                                                 \n",
      " sequential_15 (Sequential)  (None, 31, 256)           66816     \n",
      "                                                                 \n",
      " dense_127 (Dense)           multiple                  44718     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1376943 (5.25 MB)\n",
      "Trainable params: 1376431 (5.25 MB)\n",
      "Non-trainable params: 512 (2.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models.models import TransformerModel \n",
    "\n",
    "\n",
    "model = TransformerModel(\n",
    "    running_units=256,\n",
    "    d=64,\n",
    "    h=4,\n",
    "    ffn_mult=1,\n",
    "    depth=3,\n",
    "    pos_type='learned', # learned\n",
    "    prec_type=\"pretoken\", # embed_input | pretoken | inject_pre | inject_ffn\n",
    "    learned_pos=True,\n",
    "    prenorm=False,\n",
    "    norm_type=\"layer\",\n",
    "    penultimate_units=None,\n",
    "    output_units=174,\n",
    "    max_charge=6,\n",
    "    sequence_length=30,\n",
    "    alphabet=False,\n",
    "    dropout=0,\n",
    "    precursor_units=None,\n",
    "    inject_position=\"all\" # all | pre | post    \n",
    ")\n",
    "\n",
    "\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer='adam', \n",
    "            loss=masked_spectral_distance,\n",
    "            metrics=[masked_pearson_correlation_distance])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from callbacks import CyclicLR, LearningRateLogging\n",
    "\n",
    "cyclicLR = CyclicLR(base_lr=0.000001, max_lr=0.0002, step_size=2, mode='triangular',\n",
    "                 gamma=0.95)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    patience=20,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "learningRate = LearningRateLogging()\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    rt_data.tensor_train_data,\n",
    "    validation_data=rt_data.tensor_val_data,\n",
    "    epochs=4,\n",
    "#    callbacks=[\n",
    "#        WandbCallback(save_model=False),\n",
    "#        cyclicLR,\n",
    "#        early_stopping,\n",
    "        #save_best,\n",
    "#        learningRate\n",
    "#    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c191723-6855-43b1-b9b8-848d5fd7f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "intdata["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad19be0a-f48c-42dd-9447-72909158e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'modified_sequence': <tf.Tensor: shape=(2048, 30), dtype=float32, numpy=\n",
      "array([[nan,  3., 16., ...,  0.,  0.,  0.],\n",
      "       [nan, 15.,  3., ...,  0.,  0.,  0.],\n",
      "       [nan,  3., 17., ...,  0.,  0.,  0.],\n",
      "       ...,\n",
      "       [nan, 22., 16., ...,  0.,  0.,  0.],\n",
      "       [nan, 18., 10., ...,  0.,  0.,  0.],\n",
      "       [nan,  8., 17., ...,  0.,  0.,  0.]], dtype=float32)>, 'precursor_charge_onehot': <tf.Tensor: shape=(2048, 6), dtype=int64, numpy=\n",
      "array([[0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 1, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0]])>, 'collision_energy_aligned_normed': <tf.Tensor: shape=(2048,), dtype=float32, numpy=\n",
      "array([0.35      , 0.2860499 , 0.35      , ..., 0.35      , 0.28745976,\n",
      "       0.28614625], dtype=float32)>, 'method_nbr': <tf.Tensor: shape=(2048,), dtype=int64, numpy=array([2, 1, 2, ..., 2, 1, 1])>}, <tf.Tensor: shape=(2048, 174), dtype=float32, numpy=\n",
      "array([[ 0.        ,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.05      ,  0.        ,  0.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       ...,\n",
      "       [ 0.32857144,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.25      ,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.25      ,  0.        ,  0.        , ..., -1.        ,\n",
      "        -1.        , -1.        ]], dtype=float32)>)\n",
      "({'modified_sequence': <tf.Tensor: shape=(788, 30), dtype=float32, numpy=\n",
      "array([[nan, 10.,  3., ...,  0.,  0.,  0.],\n",
      "       [nan, 15., 12., ...,  0.,  0.,  0.],\n",
      "       [nan, 18., 13., ...,  0.,  0.,  0.],\n",
      "       ...,\n",
      "       [nan,  6., 17., ...,  0.,  0.,  0.],\n",
      "       [nan, 10.,  1., ...,  0.,  0.,  0.],\n",
      "       [nan, 16.,  6., ...,  0.,  0.,  0.]], dtype=float32)>, 'precursor_charge_onehot': <tf.Tensor: shape=(788, 6), dtype=int64, numpy=\n",
      "array([[0, 1, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 1, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0]])>, 'collision_energy_aligned_normed': <tf.Tensor: shape=(788,), dtype=float32, numpy=\n",
      "array([0.2860499 , 0.28572592, 0.35      , 0.28745976, 0.23675813,\n",
      "       0.35      , 0.2860499 , 0.35      , 0.35      , 0.28662485,\n",
      "       0.35      , 0.35      , 0.2870457 , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.28475964,\n",
      "       0.35      , 0.28614625, 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.2866296 ,\n",
      "       0.35      , 0.28824273, 0.35      , 0.2860499 , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.28538   , 0.28514987, 0.35      , 0.28745976, 0.35      ,\n",
      "       0.28693008, 0.28572592, 0.35      , 0.35      , 0.35      ,\n",
      "       0.28188527, 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28623992, 0.28428265, 0.28428265, 0.35      ,\n",
      "       0.35      , 0.28574175, 0.35      , 0.28498495, 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28824273, 0.28623992, 0.35      , 0.35      ,\n",
      "       0.28745976, 0.35      , 0.35      , 0.28188527, 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.28627536,\n",
      "       0.28504604, 0.2866296 , 0.28693008, 0.2834891 , 0.28538692,\n",
      "       0.35      , 0.35      , 0.35      , 0.28614625, 0.28623992,\n",
      "       0.35      , 0.28623992, 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28572592, 0.35      , 0.35      , 0.28572592,\n",
      "       0.28538   , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.28745976, 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28504604, 0.28188527, 0.35      , 0.35      ,\n",
      "       0.28574175, 0.28572592, 0.35      , 0.28824273, 0.35      ,\n",
      "       0.35      , 0.2450675 , 0.28623992, 0.35      , 0.35      ,\n",
      "       0.35      , 0.28574175, 0.35      , 0.35      , 0.28614625,\n",
      "       0.35      , 0.35      , 0.28538   , 0.35      , 0.28745976,\n",
      "       0.35      , 0.35      , 0.28623992, 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.2834891 , 0.28188527, 0.35      ,\n",
      "       0.28572592, 0.28538   , 0.28745976, 0.35      , 0.28538692,\n",
      "       0.35      , 0.35      , 0.28745976, 0.35      , 0.35      ,\n",
      "       0.35      , 0.2866296 , 0.28623992, 0.35      , 0.28538692,\n",
      "       0.28538   , 0.28693008, 0.35      , 0.28504604, 0.28614625,\n",
      "       0.28745976, 0.28504604, 0.28484738, 0.35      , 0.28428265,\n",
      "       0.35      , 0.35      , 0.28745976, 0.28574175, 0.28514987,\n",
      "       0.35      , 0.2860499 , 0.28498495, 0.35      , 0.2860499 ,\n",
      "       0.28623992, 0.35      , 0.28572592, 0.28572592, 0.2834891 ,\n",
      "       0.35      , 0.28538   , 0.35      , 0.2866296 , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35986486, 0.28627536, 0.28484738,\n",
      "       0.35      , 0.35      , 0.35      , 0.28572592, 0.28614625,\n",
      "       0.35      , 0.35      , 0.28188527, 0.28745976, 0.35      ,\n",
      "       0.28662485, 0.28514987, 0.28498495, 0.28572592, 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.2860499 ,\n",
      "       0.35      , 0.23475964, 0.28614625, 0.35      , 0.35      ,\n",
      "       0.28623992, 0.35      , 0.35      , 0.28662485, 0.35      ,\n",
      "       0.35      , 0.35      , 0.28484738, 0.35      , 0.35      ,\n",
      "       0.35      , 0.28538   , 0.35      , 0.28373727, 0.35      ,\n",
      "       0.35      , 0.28484738, 0.28745976, 0.35      , 0.35      ,\n",
      "       0.35      , 0.2860499 , 0.28538   , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28574175, 0.28662485, 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.28475964, 0.28484738, 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.28514987, 0.35      , 0.28188527, 0.35      , 0.28498495,\n",
      "       0.35      , 0.35      , 0.35      , 0.28504604, 0.35      ,\n",
      "       0.28574175, 0.28188527, 0.35      , 0.35      , 0.28428265,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28824273, 0.35      , 0.28514987, 0.28574175,\n",
      "       0.28824273, 0.28538   , 0.28627536, 0.2870457 , 0.2855086 ,\n",
      "       0.28574175, 0.28623992, 0.35      , 0.35      , 0.35      ,\n",
      "       0.2834891 , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28623992, 0.28428265, 0.35      , 0.35      ,\n",
      "       0.35      , 0.28824273, 0.35      , 0.28572592, 0.28498495,\n",
      "       0.28475964, 0.35      , 0.35      , 0.35      , 0.28514987,\n",
      "       0.35      , 0.35      , 0.35      , 0.28484738, 0.35      ,\n",
      "       0.35      , 0.28805712, 0.28498495, 0.2860499 , 0.35      ,\n",
      "       0.28538   , 0.35      , 0.28824273, 0.28824273, 0.35      ,\n",
      "       0.35      , 0.28538692, 0.35      , 0.28504604, 0.28623992,\n",
      "       0.28188527, 0.2860499 , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.28623992, 0.35      , 0.35      ,\n",
      "       0.28428265, 0.35      , 0.35      , 0.28484738, 0.35      ,\n",
      "       0.2870457 , 0.35      , 0.2870457 , 0.35      , 0.35      ,\n",
      "       0.2834891 , 0.35      , 0.35      , 0.35      , 0.28675812,\n",
      "       0.28504604, 0.35      , 0.28514987, 0.35      , 0.28693008,\n",
      "       0.35      , 0.2080571 , 0.28623992, 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.2870457 , 0.35      ,\n",
      "       0.35      , 0.2834891 , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28614625, 0.35      , 0.2866296 , 0.35      ,\n",
      "       0.35      , 0.35      , 0.28572592, 0.35      , 0.28504604,\n",
      "       0.28627536, 0.35      , 0.28538692, 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.28824273, 0.35      , 0.28514987,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.28538   , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28745976, 0.25913233, 0.28662485, 0.28572592,\n",
      "       0.2860499 , 0.35      , 0.35      , 0.35      , 0.28614625,\n",
      "       0.35      , 0.28745976, 0.28484738, 0.28745976, 0.28627536,\n",
      "       0.35      , 0.35      , 0.28504604, 0.3588376 , 0.2870457 ,\n",
      "       0.28475964, 0.28574175, 0.35      , 0.28538692, 0.35      ,\n",
      "       0.2866296 , 0.28572592, 0.35      , 0.35      , 0.35      ,\n",
      "       0.28188527, 0.2866296 , 0.28623992, 0.35      , 0.2870457 ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.2866296 ,\n",
      "       0.28498495, 0.35      , 0.35      , 0.35      , 0.28627536,\n",
      "       0.35      , 0.28188527, 0.35      , 0.35      , 0.28623992,\n",
      "       0.28538692, 0.2860499 , 0.28693008, 0.28623992, 0.28824273,\n",
      "       0.2834891 , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.28745976, 0.35      ,\n",
      "       0.35      , 0.35      , 0.28824273, 0.35      , 0.28504604,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.28504604,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.28745976, 0.28745976, 0.28188527, 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.28662485, 0.35      ,\n",
      "       0.28572592, 0.28373727, 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.28614625, 0.28538   , 0.35      ,\n",
      "       0.28662485, 0.35      , 0.35      , 0.2860499 , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.28514987, 0.28514987,\n",
      "       0.35      , 0.28662485, 0.35      , 0.35      , 0.35      ,\n",
      "       0.28627536, 0.28745976, 0.35      , 0.35      , 0.35      ,\n",
      "       0.2855086 , 0.28538692, 0.35      , 0.28514987, 0.28504604,\n",
      "       0.35      , 0.28824273, 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.28693008, 0.28572592,\n",
      "       0.35      , 0.28538   , 0.35      , 0.35      , 0.35      ,\n",
      "       0.28514987, 0.35      , 0.35      , 0.35      , 0.28745976,\n",
      "       0.28484738, 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.2834891 , 0.35      , 0.28824273, 0.28514987, 0.35      ,\n",
      "       0.35      , 0.28428265, 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.28627536, 0.35      , 0.28188527, 0.28538692, 0.28824273,\n",
      "       0.35      , 0.2860499 , 0.35      , 0.28627536, 0.2866296 ,\n",
      "       0.35      , 0.2834891 , 0.35      , 0.35      , 0.28745976,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.2834891 , 0.35      , 0.35      , 0.35      ,\n",
      "       0.28188527, 0.28662485, 0.35      , 0.28646055, 0.28538692,\n",
      "       0.35      , 0.28614625, 0.28745976, 0.35      , 0.35      ,\n",
      "       0.28745976, 0.35      , 0.28745976, 0.28484738, 0.28504604,\n",
      "       0.35      , 0.2855086 , 0.28373727, 0.2834891 , 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.28824273, 0.28504604, 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28538   , 0.35      , 0.2860499 , 0.28675812,\n",
      "       0.35      , 0.2866296 , 0.28428265, 0.35      , 0.35      ,\n",
      "       0.28504604, 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.28504604, 0.28693008, 0.2834891 , 0.2870457 , 0.2834891 ,\n",
      "       0.28514987, 0.28662485, 0.30713898, 0.35      , 0.35      ,\n",
      "       0.35      , 0.2870457 , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.28538   , 0.28538692, 0.28646055,\n",
      "       0.35      , 0.28693008, 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.35      , 0.28745976, 0.35      , 0.28614625,\n",
      "       0.28538692, 0.35      , 0.28623992, 0.35      , 0.2870457 ,\n",
      "       0.35      , 0.28498495, 0.35      , 0.28623992, 0.35      ,\n",
      "       0.28662485, 0.28188527, 0.35      , 0.28188527, 0.2866296 ,\n",
      "       0.20655958, 0.28188527, 0.2860499 , 0.2860499 , 0.35      ,\n",
      "       0.35      , 0.2860499 , 0.28824273, 0.28623992, 0.35      ,\n",
      "       0.28662485, 0.35      , 0.28627536, 0.35      , 0.35      ,\n",
      "       0.28538   , 0.35      , 0.35      , 0.35      , 0.35      ,\n",
      "       0.35      , 0.28514987, 0.28504604, 0.35      , 0.35      ,\n",
      "       0.28188527, 0.35      , 0.35      , 0.35      , 0.2866296 ,\n",
      "       0.28824273, 0.28428265, 0.35      , 0.28498495, 0.35      ,\n",
      "       0.35      , 0.28572592, 0.35      , 0.28655958, 0.35      ,\n",
      "       0.35      , 0.35      , 0.35      , 0.35      , 0.28484738,\n",
      "       0.35      , 0.35      , 0.28693008, 0.28428265, 0.28574175,\n",
      "       0.35      , 0.35      , 0.35      , 0.2860499 , 0.35      ,\n",
      "       0.35      , 0.28574175, 0.28824273, 0.28572592, 0.35      ,\n",
      "       0.35      , 0.28428265, 0.28614625, 0.28745976, 0.35      ,\n",
      "       0.28745976, 0.35      , 0.35      ], dtype=float32)>, 'method_nbr': <tf.Tensor: shape=(788,), dtype=int64, numpy=\n",
      "array([1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2,\n",
      "       2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2,\n",
      "       1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2,\n",
      "       2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1,\n",
      "       1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1,\n",
      "       1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1,\n",
      "       2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1,\n",
      "       1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2,\n",
      "       1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2,\n",
      "       2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1,\n",
      "       2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1,\n",
      "       1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "       2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1,\n",
      "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1,\n",
      "       2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2,\n",
      "       1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2,\n",
      "       1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
      "       2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1,\n",
      "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1,\n",
      "       1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1,\n",
      "       1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2,\n",
      "       2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2,\n",
      "       2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2,\n",
      "       1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
      "       2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1,\n",
      "       2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2,\n",
      "       2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2,\n",
      "       1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2,\n",
      "       1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2,\n",
      "       2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1,\n",
      "       2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2])>}, <tf.Tensor: shape=(788, 174), dtype=float32, numpy=\n",
      "array([[ 0.1025641 ,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.        ,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.07446808,  0.        ,  0.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       ...,\n",
      "       [ 0.07      ,  0.        ,  0.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.        ,  0.        , -1.        , ..., -1.        ,\n",
      "        -1.        , -1.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ..., -1.        ,\n",
      "        -1.        , -1.        ]], dtype=float32)>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m rt_data\u001b[38;5;241m.\u001b[39mtensor_train_data:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrain_data\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "ALPHABET_UNMOD = {\n",
    "    \"A\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "    \"F\": 5,\n",
    "    \"G\": 6,\n",
    "    \"H\": 7,\n",
    "    \"I\": 8,\n",
    "    \"K\": 9,\n",
    "    \"L\": 10,\n",
    "    \"M\": 11,\n",
    "    \"N\": 12,\n",
    "    \"P\": 13,\n",
    "    \"Q\": 14,\n",
    "    \"R\": 15,\n",
    "    \"S\": 16,\n",
    "    \"T\": 17,\n",
    "    \"V\": 18,\n",
    "    \"W\": 19,\n",
    "    \"Y\": 20,\n",
    "    \"M[UNIMOD:35]\": 21,\n",
    "    \"R[UNIMOD:7]\":22,\n",
    "    \"C[UNIMOD:4]\": 2,\n",
    "    \"Q[UNIMOD:7]\":4,\n",
    "    \"N[UNIMOD:7]\":3,\n",
    "}\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "string_lookup = preprocessing.StringLookup(vocabulary=list(ALPHABET_UNMOD.keys()))\n",
    "\n",
    "\n",
    "input_embedding = tf.one_hot(string_lookup([\"A\", \"S\", \"V\"]), len(ALPHABET_UNMOD))\n",
    "\n",
    "input_embedding\n",
    "\n",
    "for i in rt_data.tensor_train_data:\n",
    "    print(i)\n",
    "\n",
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3209f0f-572d-4667-9cea-993e1fcf8359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 13:04:50.900545: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 13:04:50.900601: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 13:04:50.900640: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "K = tf.keras\n",
    "L = K.layers\n",
    "A = K.activations\n",
    "I = K.initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca24beb",
   "metadata": {},
   "source": [
    "\n",
    "class AdaptiveNorm(L.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AdaptiveNorm, self).__init__()\n",
    "        self.norm = L.LayerNormalization()\n",
    "        self.norm.trainable = False\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "layer = AdaptiveNorm()\n",
    "x = tf.random.normal((10,30,512))\n",
    "\n",
    "\n",
    "layer(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13c89b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.trainable_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7eee32e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0013750743\n",
      "0.10305293\n",
      "-0.0013750743\n",
      "0.10305293\n"
     ]
    }
   ],
   "source": [
    "class AdaptiveNorm(L.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AdaptiveNorm, self).__init__()\n",
    "        self.norm = L.LayerNormalization()\n",
    "        self.norm.trainable = False\n",
    "        \n",
    "        \n",
    "    def build(self, x):\n",
    "        self.scalebias = L.Dense(2*x[-1], \n",
    "                                 use_bias=False, \n",
    "                                 kernel_initializer=I.RandomNormal(0, 0.1 * x[-1]**-0.5)) \n",
    "    \n",
    "    def call(self, x, metadata):\n",
    "         \n",
    "        scalebias = self.scalebias(metadata)  # (bs, 2*ru)\n",
    "        scale, bias = tf.split(scalebias, 2, -1) # 2 x (bs, ru)\n",
    "        \n",
    "        print(scale.numpy().mean())\n",
    "        print(scale.numpy().std())\n",
    "        \n",
    "        scale = scale[:,None]  # (bs, 1, ru)\n",
    "        bias = bias[:,None]    # (bs, 1, ru)\n",
    "        \n",
    "        out = self.norm(x)\n",
    "        out = (1 + scale) * out - bias \n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "class BaseNorm(L.Layer):\n",
    "\n",
    "    def __init__(self, norm):\n",
    "        super(BaseNorm, self).__init__()\n",
    "        self.norm = norm\n",
    "\n",
    "    def __call__(self, x, metadata=None):\n",
    "        if isinstance(self.norm, AdaptiveNorm):\n",
    "            return self.norm(x, metadata)\n",
    "        else:\n",
    "            return self.norm(x)\n",
    "\n",
    "\n",
    "    \n",
    "layer = AdaptiveNorm()\n",
    "x = tf.random.normal((10,30,256))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metadata = tf.random.normal((10,256))\n",
    "\n",
    "layer(x, metadata)\n",
    "\n",
    "layer.trainable_variables\n",
    "\n",
    "base_layer = BaseNorm(layer)\n",
    "\n",
    "out = base_layer(x, metadata)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305504b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal((10,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249df435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61f92b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0047188704"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbdfea37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.split(x, 2, -1)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:astral] *",
   "language": "python",
   "name": "conda-env-astral-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
